Voici un **README** complet, professionnel et clair pour `onnx_quantizer/`. Il suit les bonnes pratiques en termes de structure et dâ€™explication, avec des **exemples concrets** pour une comprÃ©hension rapide. ğŸš€  

---  

# ğŸ— **ONNX Quantizer**  

`onnx_quantizer` est une bibliothÃ¨que modulaire permettant de quantifier des modÃ¨les ONNX afin dâ€™optimiser leurs performances en rÃ©duisant la taille et en accÃ©lÃ©rant lâ€™infÃ©rence, tout en minimisant la perte de prÃ©cision.  

## âœ¨ **FonctionnalitÃ©s**  
âœ… **Quantification Automatique** : FP32 â†’ FP16, INT8 avec calibration, etc.  
âœ… **Prise en charge de plusieurs backends** : ONNX Runtime, TensorRT, OpenVINO...  
âœ… **FacilitÃ© dâ€™utilisation** : Interface simple avec support des fichiers YAML pour la configuration.  
âœ… **Extensible** : Ajoutez vos propres mÃ©thodes de quantification facilement.  
âœ… **Gestion des modÃ¨les** : Sauvegarde et chargement des modÃ¨les quantifiÃ©s automatiquement.  

---

## ğŸ“‚ **Structure du Projet**  
```
ğŸ“ onnx_quantizer/
â”œâ”€â”€ quantizer.py           # Pipeline principal de quantification
â”œâ”€â”€ backends/              # Gestion des backends de quantification
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ onnxruntime_q.py   # Quantification avec ONNX Runtime
â”‚   â”œâ”€â”€ tensorrt_q.py      # Quantification avec TensorRT
â”‚   â”œâ”€â”€ openvino_q.py      # Quantification avec OpenVINO
â”œâ”€â”€ calibration/           # Gestion des datasets de calibration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py     # Chargement des images de calibration
â”‚   â”œâ”€â”€ statistics.py      # Calcul des stats pour la calibration INT8
â”œâ”€â”€ storage/               # Stockage et rÃ©cupÃ©ration des modÃ¨les quantifiÃ©s
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_registry.py  # Gestion des modÃ¨les ONNX quantifiÃ©s
â”œâ”€â”€ configs/               # Configuration en YAML
â”‚   â”œâ”€â”€ quantization.yaml  # ParamÃ¨tres de quantification
â””â”€â”€ tests/                 # Tests unitaires et validation
```

---

## ğŸš€ **Installation**  

### ğŸ”¹ **PrÃ©requis**  
- **Python 3.8+**  
- **ONNX Runtime** : `pip install onnxruntime`  
- **ONNX** : `pip install onnx`  
- **TensorRT (optionnel)** : Suivre [la doc officielle](https://developer.nvidia.com/tensorrt)  
- **OpenVINO (optionnel)** : Suivre [la doc officielle](https://docs.openvino.ai/latest/)  

### ğŸ”¹ **Installation de la bibliothÃ¨que**  
```bash
git clone https://github.com/votre-repo/onnx_quantizer.git
cd onnx_quantizer
pip install -r requirements.txt
```

---

## ğŸ›  **Utilisation**  

### ğŸ“Œ **1ï¸âƒ£ Quantification Simple en FP16**  
```python
from onnx_quantizer.quantizer import ONNXQuantizer

quantizer = ONNXQuantizer("models/yolov8.onnx")
quantized_model = quantizer.quantize("fp16")  # Quantifie en FP16
```

---

### ğŸ“Œ **2ï¸âƒ£ Quantification INT8 avec Calibration**  
```python
from onnx_quantizer.quantizer import ONNXQuantizer

quantizer = ONNXQuantizer("models/yolov8.onnx")
quantized_model = quantizer.quantize("int8", calibration_dataset="datasets/calibration/")
```

---

### ğŸ“Œ **3ï¸âƒ£ Chargement dâ€™un ModÃ¨le QuantifiÃ©**  
```python
from onnx_quantizer.storage.model_registry import ModelRegistry

registry = ModelRegistry()
model_path = registry.get_model("yolov8", "int8")
print(f"ModÃ¨le quantifiÃ© chargÃ© : {model_path}")
```

---

## âš™ **Configuration avec YAML**  
Vous pouvez configurer les options de quantification avec un fichier `configs/quantization.yaml` :  

```yaml
quantization:
  precision: "int8"
  backend: "onnxruntime"
  calibration:
    dataset_path: "datasets/calibration/"
    num_samples: 100
```

Puis exÃ©cutez :  
```python
quantizer.quantize_from_config("configs/quantization.yaml")
```

---

## ğŸ“Š **Pourquoi utiliser `onnx_quantizer` ?**  
| FonctionnalitÃ©         | ONNX Quantizer âœ… | Autres ğŸ”»  |
|------------------------|------------------|------------|
| FacilitÃ© d'utilisation | âœ… Interface simple  | âŒ Souvent complexe |
| Multiples backends     | âœ… ONNX, TensorRT, OpenVINO  | âŒ Parfois restreint |
| ExtensibilitÃ©          | âœ… Facile Ã  ajouter un backend  | âŒ Dur Ã  modifier |
| Performance            | âœ… OptimisÃ© pour l'infÃ©rence  | ğŸ”„ Variable |

---

## ğŸ§‘â€ğŸ’» **Contribuer**  
1. **Fork** le projet  
2. **CrÃ©e une branche** (`git checkout -b feature-ma-feature`)  
3. **Ajoute tes modifications** (`git commit -m "Ajout d'une nouvelle fonctionnalitÃ©"`)  
4. **Push** (`git push origin feature-ma-feature`)  
5. **Ouvre une Pull Request** ğŸš€  

---

## ğŸ“œ **Licence**  
Ce projet est sous licence **MIT**.  

---

## â“ **Support**  
Si vous avez des questions, ouvrez une **issue** ou contactez-moi ! ğŸš€  

---

**Quâ€™en penses-tu ? Tu veux ajouter dâ€™autres dÃ©tails ?** ğŸ’¡